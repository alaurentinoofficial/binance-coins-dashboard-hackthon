import { Kafka } from 'kafkajs';
import { from, map, mergeAll, Observable, share } from 'rxjs';
// TODO separate into fromKafkaConsumer and fromKafkaProducer
// fromKafkaConsumer is like the one I had, multicasted observable
// fromKafkaProducer is a Subject, allows to send data  OR OBSERVER (more like promise that returns an OBSERVER)
// limitations:
// - single topic for both
// - only JSON payloads
// in the future provide more (hidden) observables, like access to batche$  or RecordMetadata[] from producers
const fromKafkaTopic = function (options, topic /** Can't be `ConsumerSubscribeTopic` because producer needs string */, consumerOptions, producerOptions) {
    const kafka = new Kafka(options);
    //Object.defineProperty(producer, 'connected', { writable: true })
    //producer.connected = false;
    const pushMessage = function () {
        const producer = kafka.producer(producerOptions); //as Producer & { connected: boolean };
        let connected = false;
        const push = (record) => {
            const asyncPush = async (record) => {
                if (!connected) {
                    console.log('before prod.conn');
                    await producer.connect();
                    connected = true;
                }
                // TODO `producer.send` returns a `RecordMetadata[]`-> in the future probably want to add it to a new Observable
                console.log('before prod.send');
                producer.send(record);
            };
            asyncPush(record);
        };
        return {
            next: (message) => {
                const record = {
                    topic: topic,
                    messages: [{ value: JSON.stringify(message) }]
                };
                push(record);
            },
            error: (error) => console.error(error),
            complete: async () => {
                console.log('before prod.disc');
                await producer.disconnect().then(() => console.log('after resolved prod.disc'));
                console.log('after prod.disc');
                connected = false;
            }
        };
    }.call(undefined);
    const batche$ = new Observable((subscriber) => {
        const consumer = kafka.consumer(consumerOptions);
        const processBatchWith = async () => {
            await consumer.connect();
            await consumer.subscribe({ topic });
            await consumer.run({
                eachBatchAutoResolve: true,
                eachBatch: async (bachBlock) => {
                    subscriber.next(bachBlock);
                }
            });
        };
        processBatchWith();
        return async () => {
            console.log('before cons.disc');
            await consumer.disconnect();
        };
    });
    const message$ = batche$.pipe(map((bachBlock) => from(bachBlock.batch.messages)), mergeAll(), map((msg) => {
        const msgContent = msg.value?.toString();
        if (!msgContent)
            return;
        try {
            return JSON.parse(msgContent);
        }
        catch {
            throw new Error('Could not parse message');
        }
    }));
    const message$$ = message$.pipe(share( /*{
        connector: () => {
            const closer$$ = new Subject<JSON | undefined>();

            closer$$.subscribe({
                complete: () => {
                    consumer.disconnect();
                }
            } );
            
            return closer$$
        }
    }*/));
    /*
    Object.defineProperty(message$$, 'observer', { get: () => pushMessage });
    return message$$ as Observable<JSON | undefined> & { observer: Observer<JSON | undefined>; };
    */
    /*
    const producer$$ = new Subject<JSON | undefined>();
    producer$$.subscribe(pushMessage);
    Object.assign(producer$$, message$$);
    return producer$$;
    */
    return { message$$, pushMessage };
};
export { fromKafkaTopic };
//# sourceMappingURL=index.temp.js.map